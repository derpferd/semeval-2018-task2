%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith


% Rules:
%This should use the official latex template from Semeval
%Your draft should be at least 3 double column pages long (excluding references).
%Your paper must not be longer than 4 pages (excluding references).
%All writing, figures, and tables must be original to your team.
% Content:
%Include an overview of the problem from your task
%A complete description of your baseline method and results
%As well as as much description as you can provide for your stage 2 approach.
%You should also include short descriptions and references to any related work you have drawn upon.

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

\usepackage{url}
\usepackage{booktabs}

\aclfinalcopy % Uncomment this line for the all SemEval submissions
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand{\winksmile}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f609.png}%
  \endgroup
}
\newcommand{\redheart}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u2764.png}%
  \endgroup
}
\newcommand{\hearteyes}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f60d.png}%
  \endgroup
}
\newcommand{\joytears}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f602.png}%
  \endgroup
}
\newcommand{\smileeyes}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f60a.png}%
  \endgroup
}
\newcommand{\smileshades}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f60e.png}%
  \endgroup
}
\newcommand{\purpleheart}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f49c.png}%
  \endgroup
}
\newcommand{\twohearts}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f495.png}%
  \endgroup
}
\newcommand{\blowingkiss}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f618.png}%
  \endgroup
}
\newcommand{\music}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f3b6.png}%
  \endgroup
}
\newcommand{\camera}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f4f7.png}%
  \endgroup
}
\newcommand{\cameraflash}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/emoji_u1f4f8.png}%
  \endgroup
}
\newcommand{\esflag}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/ES.png}%
  \endgroup
}
\newcommand{\usflag}{%
  \begingroup\normalfont
  \includegraphics[height=\fontcharht\font`\B]{emojis/US.png}%
  \endgroup
}

%Title format for system description papers by task participants
\title{UMDuluth-CS8761 at SemEval-2018 Task 2:\\
Emojis: Too many Choices? \winksmile}
%Title format for task description papers by task organizers
%\title{SemEval-2017 Task [TaskNumber]:  [Task Name]}


\author{
  Dennis Asamoah Owusu \& Jonathan Beaulieu \\
  Department of Computer Science \\
  University of Minnesota Duluth \\
  Duluth, MN 55812 USA \\
  {\tt \{asamo012,beau0307\}@d.umn.edu} \\
  {\tt https://github.umn.edu/beau0307/semeval-2018-task2} \\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this paper, we present our system for assigning an emoji to a tweet based on the text. Each of the tweets was originally posted with an emoji which the task providers have removed and our task is to decide out of 20 emojis, the emoji that originally came with the tweet. Two datasets are provided - one in English and the other in Spanish. We treat the task as a standard classification task with the emojis as our classes and the tweets as our documents. Our best performing system uses a Bag of Words model with a Linear Support Vector Machine as classifier. We achieve a macro F1 score of 32.73 for the English data and 17.98 for the Spanish data.
   
\end{abstract}

\section{Introduction}

An AI system that can associate text with appropriate emojis could be useful for generating content that is sparkled with emojis among other uses \cite{Barb:17}. Given only the text from a tweet in English or Spanish, the task (from Semeval-2018) is to determine the emoji that was in the original tweet. To learn how users associate emojis with text, a dataset comprising 489,609 tweets in English and 98,289 tweets in Spanish is provided. Each tweet has a corresponding label representing the emoji that was in the tweet. The total number of labels was 20 for the English data and 20 for the Spanish data. We treat the task as a standard classification task with the emojis as our classes and the tweets as our documents. We view the emojis as approximations of the sentiment expressed in the text. 

For our baseline, we implemented a Bag of Words model using a Bernoulli Naive Bayes classifier. We analyzed the results, performed some variations of the baseline to gain more insights into the results and then used those insights to implement our final system. Our final system also used a Bag of Words model. A Linear Support Vector machine was used for classification. This system performed better than our baseline by ~3.5 percentage points for our English data and ~1.5 percentage points for our Spanish data. It also performed better than several neural network models we experimented with. Our macro F1 score were ~32.73 and ~17.98 for our English data and Spanish data respectively.

\section{Baseline}

For our Baseline, we used a Bag of Words model (BOW) with a Bernoulli Naive Bayes Classifier. We also implemented a Most Frequent Class Model (MFC) and a Random Model (RAND) to help draw insights from our baseline. The results of these models for the English and Spanish data are shown in Table \ref{table:baseline-en} and Table \ref{table:baseline-es} respectively. The tables show mean and standard deviation since we evaluated the models using 10-fold cross-validation. We reserved 10\% of the data for testing and trained on the remaining 90\% for each fold. We follow the same approach in all our model experiments. The Micro F1 scores are heavily influenced by the performance of the dominant classes. Since ~21\% of the tweets belong to label 0 for English, the micro F1 score was 21\% for the Most Frequent Class model. Macro F1 scores, on the other hand, since they average the F1 scores of each represented class, give a more or less equal weight to each class. 

\begin{table}[]
\centering

\begin{tabular}{@{}l|l|l|l|l|@{}}
\cmidrule(l){2-5}
                                    & \multicolumn{2}{c|}{\textbf{Macro F1}} & \multicolumn{2}{c|}{\textbf{Micro F1}} \\ \cmidrule(l){2-5} 
                                    & \textbf{Mean}     & \textbf{S-Dev}     & \textbf{Mean}     & \textbf{S-Dev}     \\ \midrule
\multicolumn{1}{|l|}{\textbf{BOW}}  & 29.1              & 0.2                & 42.1                & 0.3                 \\ \midrule
\multicolumn{1}{|l|}{\textbf{MFC}}  & 1.8               & 0.0                  & 21.7                & 0.2                 \\ \midrule
\multicolumn{1}{|l|}{\textbf{RAND}} & 4.5               & 0.1                & 5.0                & 0.1                 \\ \bottomrule
\end{tabular}
\caption{Baseline results for English}
\label{table:baseline-en}
\end{table}

We picked a Bernoulli style Naive Bayes because it generally works better for short texts (e.g. Tweets) than its Multinomial counterpart \cite[p.~268]{Manning2008}. We empirically verified this is the case given our task and data. To implement this model, we used the NLTK library for preprocessing and the SKLearn framework for the model training \cite{BirdKleinLoper09} \cite{scikit-learn}. Our data pipeline consisted of four steps: Tokenization, Bagination, Tf-idf transform and Training. 

For Tokenization we used NLTK's TweetTokenizer function. We configured it to convert all text to lowercase, crop repeating characters to a max of 3 and remove tweeter handles. After creating a list of tokens from the tweets' text, we removed all positional data through bagination. The next step was to normalize the frequency data into term-frequency times inverse document-frequency(tf-idf) representation. Finally, we trained a Bernoulli Naive Bayes classifier on this data.

\subsection{Insights from BOW English Results}
  For the English tweets, the difference in the macro and micro F-scores for our Bernoulli model lies in the fact that about 21\% of tweets have label 0 (representing \redheart{}  - the red heart) which is the most frequent class. This can be seen from the results of our Most Frequent Class model which has a micro F1 score of roughly 21\% as shown in Table \ref{table:baseline-en}. 

Table \ref{table:confusion-matrix1} shows a confusion matrix for some labels of our BOW results for the English data. What the labels represent is shown in Table \ref{table:emojis1}. The matrix illustrates the struggles of the BOW model.  Generally a lot of tweets are misclassified as 1 and 2. Take label 6 for instance. While only 332 tweets were correctly labeled, 558 tweets and 385 tweets that should have been labeled 6 were labeled 1 and 2 respectively. This misclassification as Label 1 or Label 2 is the case not just for the classes represented in the matrix but for nearly all the 20 classes. Labels 5 and 1 illustrate another pattern we observed. 653 out of 2312 tweets that should have been classified as label 5 were misclassified as label 1. Since label 5 is Smiling face with sunglasses and label 1 is Smiling face with heart eyes, we inferred that the model might have trouble discriminating between classes with similar semantics.



\begin{table}[]
\centering

\begin{tabular}{@{}l|l|l|l|l|@{}}
\cmidrule(l){2-5}
                                    & \multicolumn{2}{c|}{\textbf{Macro F1}} & \multicolumn{2}{c|}{\textbf{Micro F1}} \\ \cmidrule(l){2-5} 
                                    & \textbf{Mean}     & \textbf{S-Dev}     & \textbf{Mean}     & \textbf{S-Dev}     \\ \midrule
\multicolumn{1}{|l|}{\textbf{BOW}}  & 16.5              & 0.4                & 29.6                & 0.4                 \\ \midrule
\multicolumn{1}{|l|}{\textbf{MFC}}  & 1.7               & 0.0                  & 20.0                & 0.4                 \\ \midrule
\multicolumn{1}{|l|}{\textbf{RAND}} & 4.8               & 0.2                & 5.5                & 0.2                 \\ \bottomrule
\end{tabular}
\caption{Baseline results for Spanish}
\label{table:baseline-es}
\end{table}


That inference is supported by the fact that the model also struggles to discriminate between labels 10 (Camera) and 18 (Camera with flash). 458 tweets out of 1344 tweets (34\%) that should have been labeled as 18 where incorrectly labeled as 10. This is the more significant when one notes that only 148 of Label 18 tweets (11\%) where labeled correctly. 

To measure the impact of these semantically similar classes on our model, we collapsed classes that were semantically similar and rerun the BOW model. All the “heart” emojis were put into one class; the “smiling” emojis (except the hearty eyes one) were put in another class; the “camera” emojis were put in another class; and each remaining emoji had its own class. In the end, we had 13 classes. 

After running the model on the 13 classes, the macro F1 score improved by 6 points suggesting that the semantic similarity between classes such as Camera and Camera with Flash does have an effect although the effect was not nearly as significant as we had expected. It is worth noting that after collapsing the semantically similar classes, a lot of tweets were misclassified as the most frequent class. Thus, it may be that the semantic similarity of the classes does really matter but the gain in performance from collapsing the classes was offset by the fact that we now had a class that was really huge and ate up too many classifications. 

The BOW performed relatively well for Labels 0 (\redheart Red heart), 1 (Smiling face with hearteyes), 2 (Face with tears of joy), 4 (Fire) and 17 (Christmas tree) for the English data.  


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}l|l|l|l|l|l|l|l|@{}}
\cmidrule(l){2-7}
 & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{5} & \textbf{6} &  \\ \midrule
\multicolumn{1}{|l|}{\textbf{0}} & 8637 & 618 &  &  &  &   10752 \\ \midrule
\multicolumn{1}{|l|}{\textbf{1}} &  & 2358 & 832 &  &  &   5145 \\ \midrule
\multicolumn{1}{|l|}{\textbf{2}} &  & 1022 & 2718 &  &  &   5114 \\ \midrule
\multicolumn{1}{|l|}{\textbf{5}} &  & 653 & 437 & 251 & 139 &   2313 \\ \midrule
\multicolumn{1}{|l|}{\textbf{6}} &  & 558 & 385 &  & 332 &   2092 \\ \midrule
\end{tabular}
\caption{BOW Confusion Matrix for English}
\label{table:confusion-matrix1}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Label} & \textbf{Emoji} & \textbf{Description} \\ \midrule
0 & \redheart & Red heart \\ \midrule
1 & \hearteyes & Smiling face with hearteyes \\ \midrule
2 & \joytears & Face with tears of joy \\ \midrule
5 & \smileeyes & Smiling face with smiling eyes \\ \midrule
6 & \smileshades & Smiling face with sunglasses \\ \midrule
13 & \purpleheart & Purple Heart \\ \midrule
10 & \camera & Camera \\ \midrule
18 & \cameraflash & Camera with Flash \\ \bottomrule
\end{tabular}
\caption{Some English Emojis}
\label{table:emojis1}
\end{table}

\subsection{Insights from BOW Spanish Results}
The Spanish results were much worse than the English results. Table \ref{table:results-es} illustrates the major trend we noticed with the BOW model's performance on the Spanish data. Total is the number of tweets that correctly belong to a class. \%C is what percentage of tweets were correctly labeled as belonging to that class. \% 0, \% 1 and \% 2 are the percentages of tweets that our model misclassified as labels 0, 1 and 2 respectively. Thus for Label 0, there were 1882 tweets in the test data; 62\% of these were labeled correctly while 14\% was wrongly labeled as 1. As the table shows, a significant percentage of tweets were either misclassified as 0, 1 or 2. The exception to this is Label 9 - the Spanish flag. Table \ref{table:emojis-es} shows the emojis corresponding to the labels. 

\begin{table}[]
\centering
\begin{tabular}{llllll}
\hline
\textbf{Label} & \textbf{\%C} & \textbf{\%0} & \textbf{\%1} & \textbf{\%2} & \textbf{Total} \\ \hline
\textbf{0} & 62 &  & 14 &  & 1882 \\ \hline
\textbf{1} & 43 & 16 &  & 14 & 1338 \\ \hline
\textbf{2} & 46 &  & 25 &  & 908 \\ \hline
\textbf{3} & 15 & 32 & 24 &  & 641 \\ \hline
\textbf{4} & 9 & 14 & 32 & 17 & 647 \\ \hline
\textbf{5} & 11 & 22 & 29 & 11 & 438 \\ \hline
\textbf{9} & 63 & 5 & 17 & 4 & 331 \\ \hline
\textbf{10} & 7 & 12 & 30 & 20 & 332 \\ \hline
\textbf{16} & 24 & 13 & 19 & 27 & 283 \\ \hline
\end{tabular}
\caption{BOW Spanish results}
\label{table:results-es}
\end{table}

\subsection{Neural Network}
Upon reading about how neural models achieved high scores on similar tasks, we decided to try out methods based on Barbieri et al. and dos Santos et al. \cite{Barb:17} \cite{Dos:14}. The task this paper tries to solve is based on Barbieri and others paper where they do the same task. Their best performing model was a character based Bi-directional Long Short-term Memory Networks (char-BLSTM). We also took inspiration from dos Santos and others. They got very good results using a Convolutional Neural Network(CNN) to do sentiment analysis. We tested four different types of neural network models: LSTM, BLSTM, CNN-LSTM, CNN-BLSTM. For the LSTMs, we used a network size of 128. The only difference between our LSTM and our BLSTM is that we added a layer to train each input bidirectionally. Our CNN's convolution layer had an output dimension of 64 and a kernel size of 5 and for it's pooling layer we chose a pool size of 4. When training each of the neural network models we used a dev set which was 10\% of the training set to pick the best model and to get an idea of how many epochs to run. We settled on these specific parameters after trying out different parameters on the development set. None of our neural network models performed significantly better than our baseline.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Label} & \textbf{Emoji} & \textbf{Description} \\ \midrule
0 & \redheart & Red heart \\ \midrule
1 & \hearteyes & Smiley with heart eyes \\ \midrule
2 & \joytears & Face with tears of joy \\ \midrule
3 & \twohearts & Two hearts \\ \midrule
4 & \smileeyes & Smiley with smiling eyes \\ \midrule
5 & \blowingkiss & Face blowing a kiss \\ \midrule
9 & \esflag & Spain \\ \midrule
10 & \smileshades & Smiling face with sunglasses \\ \midrule
16 & \music & Musical notes \\ \bottomrule
\end{tabular}
\caption{Some Spanish Emojis}
\label{table:emojis-es}
\end{table}

\subsection{Linear SVM}
Realizing that our neural network models did not perform any better than our BOW baseline, we decided to try a BOW model with another classifier which is not a neural network. We settled on a Linear Support Vector Machine. To enable multi-class classification we used a one-vs-rest approach. 

\subsection{Sampling}
Roughly 20\% of the tweets in the English data belong to Label 0. The performance of classifiers such as Naive Bayes degrades when there is such a dominant class \cite{Rennie03}. This data imbalance exists in the Spanish data as well. To improve the performance of our classifiers, we perform a sampling of the data so that we train on a data set where the classes are roughly equally represented. We perform a simple under sampling by randomly selecting an equal number of tweets from each class even though a more sophisticated re-sampling method will likely improve the results \cite{Estabrooks:Resampling}. 


\section{Results}
% \begin{table}[]
% \centering
% \begin{tabular}{@{}l|l|l|l|l|@{}}
% \cmidrule(l){2-5}
%                                     & \multicolumn{2}{c|}{\textbf{Macro F1}} & \multicolumn{2}{c|}{\textbf{Micro F1}} \\ \cmidrule(l){2-5} 
%                                     & \textbf{Mean}     & \textbf{S-Dev}     & \textbf{Mean}     & \textbf{S-Dev}     \\ \midrule
% \multicolumn{1}{|l|}{\textbf{BOW}}  & 16.5              & 0.4                & 29.6                & 0.4                 \\ \midrule
% \multicolumn{1}{|l|}{\textbf{MFC}}  & 1.7               & 0.0                  & 20.0                & 0.4                 \\ \midrule
% \multicolumn{1}{|l|}{\textbf{RAND}} & 4.8               & 0.2                & 5.5                & 0.2                 \\ \bottomrule
% \end{tabular}
% \caption{Results for Spanish}
% \label{table:results-es}
% \end{table}

\begin{table}[]
\centering
\begin{tabular}{@{}l|l|l|l|l|@{}}
\cmidrule(l){2-5}
                                    & \multicolumn{2}{c|}{\textbf{English}} & \multicolumn{2}{c|}{\textbf{Spanish}} \\ \cmidrule(l){2-5} 
                                    & \textbf{Mean}     & \textbf{S-Dev}     & \textbf{Mean}     & \textbf{S-Dev}     \\ \midrule
\multicolumn{1}{|l|}{\textbf{Base}}  & 29.10 & 0.20 & 16.49 & 0.42 \\ \midrule
\multicolumn{1}{|l|}{\textbf{C+L}}  & 29.30 & 0.36 & - & - \\ \midrule
\multicolumn{1}{|l|}{\textbf{C+L(f)}}  & 29.35 & 0.44 & - & - \\ \midrule
\multicolumn{1}{|l|}{\textbf{LSVM}}  & 32.73 & 0.24 & 17.98 & 0.31 \\ \bottomrule
\end{tabular}
\caption{Macro F1 scores. Base is baseline, L is LSTM, C is CNN, (f) means each class was equally represented.}
\label{table:results-en}
\end{table}

The neural network models that we ran ended up achieving around the same score as our BOW baseline. 
The BOW model with a Linear Support Vector Machine for classification provided the best results. Table \ref{table:results-en} shows the results of the model. We also show the results of our baseline and our best performing neural network models for comparison. The effect of sampling was negligible as shown in Table \ref{table:results-en}. The improvement to employing sampling was 0.05 percentage points for our CNN combined with LSTM model. The F1 score of our LSVM model on the test data from the task organizers was 31.834 which is within one percent of the 32.73 from our 10-fold cross-validation. Precision on the test data was 39.803, recall was 31.365 and accuracy was 45.732.


\section{Discussion}

The first most important trend we observe with our system (BOW model with LSVM classifier) is that labels 0, 1 and 2 perform pretty well in terms of true positives - ~83\% for label 0 (8848/10622), ~57\% for label 1 (2903/5077) and ~63\% for label 2 (3171/5067) while at the same time being false positives for many classes. Take label 5 for instance. 327 tweets are correctly classified as belonging to label 5. However, 732 tweets that should have been classified as label 5 are misclassified as label 1. The trend of misclassifying more tweets as label 1 was seen for labels 6, 7, 8, 13, 14, 15, 16 and 19 as well. Our baseline had this same problem. The problem trends in our baseline persisted in our final system; the final system performs better only because of marginal improvements in the classification itself.

Below are some tweets for label 18 (camera with flash) that the LSVM succeed in classifying that the Bernoulli Naive Bayes could not find. We choose label 18 because the percentage difference in performance (in favor of the LSVM) is greatest here.

\begin{quote}
Different angles to the same goal. by @user @ New…
\end{quote}

\begin{quote}
When iris.apfel speaks...knowledge and wisdom is all you hear so listen up... :@drummondphotog…
\end{quote}

Our supposition is that the Linear Support Vector Machine is able to make associations that the Bernoulli Naive Bayes is unable to make. "Angles", we suspect, correlates with camera than the other emojis and the LSVM finds that association. The second tweet is interesting because it will seem that the LSVM is able to connect the photog in @drummondphotog despite our use of a Bag of Words Model unless the prediction was based on some other, less obvious, word in the tweet.

\section*{Acknowledgments}

This project was carried out as a part of CS 8761, Natural Language Processing, offered in Fall 2017 at the University of Minnesota, Duluth by Dr. Ted Pedersen. We are grateful to Dr. Ted Pedersen for his support and guidance in this project. Authors are listed in alphabetical order.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{semeval2017}
\bibliographystyle{acl_natbib}

\end{document}
